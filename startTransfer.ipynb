{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import utils\n",
    "from basic_fcn import *\n",
    "from dataloader import *\n",
    "from transfer import *\n",
    "from utils import *\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        torch.nn.init.xavier_uniform_(m.bias.data.view(m.bias.data.shape[0],1))\n",
    "        #a = math.sqrt(3) * math.sqrt(2/m.bias.data.shape[0])\n",
    "        #torch.nn.init._no_grad_uniform_(m.bias.data, -a, a)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def train(model, criterion, epochs, train_loader, val_loader, test_loader, use_gpu, name, debug=False):\n",
    "    if debug:\n",
    "        initMem = {}\n",
    "        print(\"Initialized with\")\n",
    "        initMem = checkM(initMem)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #Create non-existing logfiles\n",
    "    logname = 'logfile_transfer.txt'\n",
    "    logname_summary = 'logfile_transfer_summary.txt'    \n",
    "    \n",
    "    if os.path.exists('logfile_transfer.txt') == True:\n",
    "        i = 1\n",
    "        logname = 'logfile_transfer' + str(i) + '.txt'\n",
    "        while os.path.exists('logfile_transfer' + str(i) + '.txt'):\n",
    "            i+=1\n",
    "            logname = 'logfile_transfer' + str(i) + '.txt'\n",
    "        logname_summary = 'logfile_transfer' + str(i) + '_summary.txt'    \n",
    "\n",
    "    print('Loading results to logfile: ' + logname)\n",
    "    with open(logname, \"a\") as file:\n",
    "        file.write(\"Lofile Transfer DATA: Validation Loss and Accuracy\\n\") \n",
    "    \n",
    "    print('Loading Transfer Summary to : ' + logname_summary) \n",
    "    pickle_file = logname_summary[::-4] +'.pkl'\n",
    "    \n",
    "        \n",
    "        \n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-3)\n",
    "    if use_gpu:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        model.to(device)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Use GPU:  \")\n",
    "        initMem = checkM(initMem)\n",
    "        \n",
    "    \n",
    "    val_loss_set = []\n",
    "    val_acc_set = []\n",
    "    val_iou_set = []\n",
    "\n",
    "    training_loss = []\n",
    "    \n",
    "    \n",
    "    # Early Stop criteria\n",
    "    minLoss = 1e6\n",
    "    minLossIdx = 0\n",
    "    earliestStopEpoch = 10\n",
    "    earlyStopDelta = 5\n",
    "    for epoch in range(epochs):\n",
    "        ts = time.time()\n",
    "             \n",
    "                  \n",
    "        if debug:\n",
    "            singleM = {}\n",
    "            \n",
    "        for iter, (inputs, tar, labels) in enumerate(train_loader):\n",
    "            \n",
    "            #print(inputs.shape)\n",
    "            optimizer.zero_grad()\n",
    "            #print(\"\\n$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "            #startMem = checkM(startMem)\n",
    "            #singleM = checkM(singleM)\n",
    "            \n",
    "            del tar\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs = inputs.to(device)# Move your inputs onto the gpu\n",
    "                labels = labels.to(device) # Move your labels onto the gpu\n",
    "            \n",
    "                \n",
    "            outputs = model(inputs)\n",
    "            del inputs\n",
    "            loss = criterion(outputs, Variable(labels.long()))\n",
    "            del labels\n",
    "            del outputs\n",
    "            \n",
    "            if debug:\n",
    "                print(\"\\n**********************************************\\nPost Loss\")\n",
    "                #backMem = checkM(backMem, True)\n",
    "                singleM = checkM(singleM)\n",
    "                #print(\"start vs back diff\")\n",
    "                #memDiff(startMem, backMem)\n",
    "            loss.backward()\n",
    "            loss = loss.item()\n",
    "            \n",
    "            if debug:\n",
    "                print(\"\\n**********************************************\\nPost Backward\")\n",
    "                #postLossMem = checkM(postLossMem, True)\n",
    "                #print(\"Post loss vs back diff\")\n",
    "                singleM = checkM(singleM)\n",
    "                #memDiff(backMem, postLossMem)\n",
    "                \n",
    "            optimizer.step()\n",
    "            \n",
    "            if debug:\n",
    "                print(\"\\n**********************************************\\nPost Step\")\n",
    "                singleM = checkM(singleM)\n",
    "                #finalMem = checkM(finalMem, True)\n",
    "                #memDiff(postLossMem, finalMem)\n",
    "            \n",
    "\n",
    "            if iter % 10 == 0:\n",
    "                print(\"epoch{}, iter{}, loss: {}\".format(epoch, iter, loss))\n",
    "                break\n",
    "            \n",
    "            if debug:    \n",
    "                print(\"\\n**********************************************\\nFinal\")\n",
    "                singleM = checkM(singleM)\n",
    "            \n",
    "                print(\"\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\")\n",
    "        \n",
    "        print(\"Finish epoch {}, time elapsed {}\".format(epoch, time.time() - ts))\n",
    "        \n",
    "        # calculate val loss each epoch\n",
    "        val_loss, val_acc, val_iou = val(model, val_loader, criterion, use_gpu)\n",
    "        val_loss_set.append(val_loss)\n",
    "        val_acc_set.append(val_acc)\n",
    "        val_iou_set.append(val_iou)\n",
    "\n",
    "        training_loss.append(loss)\n",
    "        \n",
    "        \n",
    "        with open(logname, \"a\") as file:\n",
    "            file.write(\"writing!\\n\")\n",
    "            file.write(\"Finish epoch {}, time elapsed {}\".format(epoch, time.time() - ts))\n",
    "            file.write(\"\\n training Loss:   \" + str(loss))\n",
    "            file.write(\"\\n Validation Loss: \" + str(val_loss_set[-1]))\n",
    "            file.write(\"\\n Validation acc:  \" + str(val_acc_set[-1]))\n",
    "            file.write(\"\\n Validation iou:  \" + str(val_iou_set[-1]) + \"\\n \")\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < minLoss:\n",
    "            # Store new best\n",
    "            torch.save(model, name)\n",
    "            minLoss = val_loss\n",
    "            minLossIdx = epoch\n",
    "            \n",
    "        # If passed min threshold, and no new min has been reached for delta epochs\n",
    "        elif epoch > earliestStopEpoch and (epoch - minLossIdx) > earlyStopDelta:\n",
    "            print(\"Stopping early at {}\".format(minLossIdx))\n",
    "            \n",
    "        # TODO what is this for?\n",
    "        #model.train()\n",
    "        \n",
    "        with open(logname_summary, \"w\") as file:\n",
    "            file.write(\"Summary!\\n\")\n",
    "            file.write(\"Stopped early at {}\".format(minLossIdx))\n",
    "            file.write(\"\\n training Loss:   \" + str(training_loss))        \n",
    "            file.write(\"\\n Validation Loss: \" + str(val_loss_set))\n",
    "            file.write(\"\\n Validation acc:  \" + str(val_acc_set))\n",
    "            file.write(\"\\n Validation iou:  \" + str(val_iou_set) + \"\\n \")\n",
    "\n",
    "        \n",
    "        with open(pickle_file, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "            pickle.dump([training_loss, val_loss_set, val_acc_set,val_iou_set], f)\n",
    "            \n",
    "            \n",
    "        \n",
    "    return val_loss_set, val_acc_set, val_iou_set\n",
    "\n",
    "\n",
    "def val(model, val_loader, criterion, use_gpu):\n",
    "    \n",
    "    # set to evaluation mode \n",
    "    model.eval()\n",
    "\n",
    "    softmax = nn.Softmax(dim = 1)\n",
    "    \n",
    "    loss = []\n",
    "    pred = []\n",
    "    acc = []\n",
    "    \n",
    "    IOU_init = False\n",
    "    if use_gpu:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        \n",
    "        #model.to(device)\n",
    "        \n",
    "    for iter, (inputs, tar, labels) in tqdm(enumerate(val_loader)):\n",
    "        \n",
    "        if not IOU_init:\n",
    "            IOU_init = True\n",
    "            IOU = np.zeros((1,19))\n",
    "        del tar\n",
    "        \n",
    "        if use_gpu:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "\n",
    "            \n",
    "        with torch.no_grad():   \n",
    "            outputs = model(inputs)  \n",
    "            del inputs\n",
    "            loss.append(criterion(outputs, labels.long()).item())\n",
    "            prediction = softmax(outputs) \n",
    "            del outputs\n",
    "            acc.append(pixel_acc(prediction, labels))\n",
    "            IOU = IOU + np.array(iou(prediction, labels))\n",
    "            del prediction\n",
    "            del labels\n",
    "        \n",
    "    \n",
    "    acc = sum(acc)/len(acc)\n",
    "    avg_loss = sum(loss)/len(loss) \n",
    "    IOU = IOU/iter  \n",
    "    \n",
    "    return avg_loss, acc, IOU      \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "def test(model, use_gpu):\n",
    "    \n",
    "    softmax = nn.Softmax(dim = 1)\n",
    "    \n",
    "    pred = []\n",
    "    acc = []\n",
    "    if use_gpu:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        \n",
    "        model.to(device)\n",
    "    \n",
    "    IOU_init = False\n",
    "    for iter, (X, tar, Y) in enumerate(test_loader):\n",
    "        \n",
    "        if not IOU_init:\n",
    "            IOU_init = True\n",
    "            IOU = np.zeros((1,tar.shape[1]))\n",
    "        \n",
    "        if use_gpu:\n",
    "            inputs = X.to(device)\n",
    "            labels = Y.to(device)\n",
    "        else:\n",
    "            inputs, labels = X, Y\n",
    "                    \n",
    "        \n",
    "        outputs = model(inputs)  \n",
    "        \n",
    "        prediction = softmax(outputs)\n",
    "        acc.append(pixel_acc(prediction, labels))\n",
    "        IOU = IOU + np.array(iou(prediction, Y))\n",
    "        \n",
    "    acc = sum(acc)/len(acc)        \n",
    "    IOU = IOU/iter\n",
    "\n",
    "    #Complete this function - Calculate accuracy and IoU \n",
    "    # Make sure to include a softmax after the output from your model\n",
    "    \n",
    "    return acc, IOU\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        news = self.shape.copy()\n",
    "        news[0] =  x.shape[0]\n",
    "        \n",
    "        return x.view(*tuple(news))\n",
    "\n",
    "def getTransferModel(n_class):\n",
    "    \n",
    "    decoder = nn.Sequential(\n",
    "        Reshape([1,512,32,64]),\n",
    "        \n",
    "        nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ),\n",
    "        \n",
    "        nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ),\n",
    "            \n",
    "        nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ),\n",
    "            \n",
    "        nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ),\n",
    "        \n",
    "            \n",
    "        nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ),\n",
    "\n",
    "        nn.Conv2d(32, n_class, kernel_size=1)\n",
    "        )\n",
    "    decoder.apply(init_weights)\n",
    "        \n",
    "\n",
    "    \n",
    "    model = models.resnet34(pretrained=True)\n",
    "        \n",
    "    for param in model.parameters():\n",
    "        # False implies no retraining\n",
    "        param.requires_grad=False\n",
    "        \n",
    "    del param\n",
    "\n",
    "    model.avgpool = nn.Identity() \n",
    "        \n",
    "    model.fc = decoder\n",
    "    #print(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results to logfile: logfile_transfer10.txt\n",
      "Loading Transfer Summary to : logfile_transfer10_summary.txt\n",
      "epoch0, iter0, loss: 3.830095052719116\n",
      "Finish epoch 0, time elapsed 19.315083742141724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:08,  8.39s/it]\u001b[A\n",
      "2it [00:08,  6.05s/it]\u001b[A\n",
      "3it [00:12,  5.38s/it]\u001b[A\n",
      "4it [00:13,  3.94s/it]\u001b[A\n",
      "5it [00:15,  3.33s/it]\u001b[A\n",
      "6it [00:15,  2.51s/it]\u001b[A\n",
      "7it [00:17,  2.33s/it]\u001b[A\n",
      "8it [00:18,  1.81s/it]\u001b[A\n",
      "9it [00:20,  1.81s/it]\u001b[A\n",
      "10it [00:20,  1.44s/it]\u001b[A\n",
      "11it [00:23,  1.91s/it]\u001b[A\n",
      "12it [00:24,  1.55s/it]\u001b[A\n",
      "13it [00:25,  1.52s/it]\u001b[A\n",
      "14it [00:26,  1.21s/it]\u001b[A\n",
      "15it [00:27,  1.29s/it]\u001b[A\n",
      "16it [00:28,  1.08s/it]\u001b[A\n",
      "17it [00:30,  1.32s/it]\u001b[A\n",
      "18it [00:31,  1.14s/it]\u001b[A\n",
      "19it [00:33,  1.54s/it]\u001b[A\n",
      "20it [00:34,  1.24s/it]\u001b[A\n",
      "21it [00:35,  1.29s/it]\u001b[A\n",
      "22it [00:36,  1.06s/it]\u001b[A\n",
      "23it [00:37,  1.19s/it]\u001b[A\n",
      "24it [00:38,  1.01s/it]\u001b[A\n",
      "25it [00:39,  1.14s/it]\u001b[A\n",
      "26it [00:40,  1.02it/s]\u001b[A\n",
      "27it [00:41,  1.20s/it]\u001b[A\n",
      "28it [00:42,  1.00s/it]\u001b[A\n",
      "29it [00:43,  1.12s/it]\u001b[A\n",
      "30it [00:44,  1.02it/s]\u001b[A\n",
      "31it [00:46,  1.18s/it]\u001b[A\n",
      "32it [00:46,  1.00s/it]\u001b[A\n",
      "33it [00:48,  1.11s/it]\u001b[A\n",
      "34it [00:48,  1.04it/s]\u001b[A\n",
      "35it [00:50,  1.18s/it]\u001b[A\n",
      "36it [00:50,  1.01s/it]\u001b[A\n",
      "37it [00:52,  1.17s/it]\u001b[A\n",
      "38it [00:53,  1.01it/s]\u001b[A\n",
      "39it [00:54,  1.18s/it]\u001b[A\n",
      "40it [00:55,  1.00it/s]\u001b[A\n",
      "41it [00:56,  1.17s/it]\u001b[A\n",
      "42it [00:57,  1.00s/it]\u001b[A\n",
      "43it [00:59,  1.19s/it]\u001b[A\n",
      "44it [00:59,  1.01it/s]\u001b[A\n",
      "45it [01:01,  1.11s/it]\u001b[A\n",
      "46it [01:01,  1.03it/s]\u001b[A\n",
      "47it [01:03,  1.12s/it]\u001b[A\n",
      "48it [01:03,  1.07it/s]\u001b[A\n",
      "49it [01:05,  1.09s/it]\u001b[A\n",
      "50it [01:05,  1.06it/s]\u001b[A\n",
      "51it [01:07,  1.23s/it]\u001b[A\n",
      "52it [01:08,  1.04s/it]\u001b[A\n",
      "53it [01:09,  1.19s/it]\u001b[A\n",
      "54it [01:10,  1.00s/it]\u001b[A\n",
      "55it [01:11,  1.13s/it]\u001b[A\n",
      "56it [01:12,  1.04it/s]\u001b[A\n",
      "57it [01:13,  1.12s/it]\u001b[A\n",
      "58it [01:14,  1.04it/s]\u001b[A\n",
      "59it [01:16,  1.28s/it]\u001b[A\n",
      "60it [01:16,  1.06s/it]\u001b[A\n",
      "61it [01:18,  1.16s/it]\u001b[A\n",
      "62it [01:18,  1.01s/it]\u001b[A\n",
      "63it [01:20,  1.19s/it]\u001b[A\n",
      "64it [01:21,  1.01it/s]\u001b[A\n",
      "65it [01:22,  1.16s/it]\u001b[A\n",
      "66it [01:23,  1.00it/s]\u001b[A\n",
      "67it [01:24,  1.21s/it]\u001b[A\n",
      "68it [01:25,  1.02s/it]\u001b[A\n",
      "69it [01:27,  1.18s/it]\u001b[A\n",
      "70it [01:27,  1.01it/s]\u001b[A\n",
      "71it [01:29,  1.15s/it]\u001b[A\n",
      "72it [01:29,  1.02it/s]\u001b[A\n",
      "73it [01:31,  1.17s/it]\u001b[A\n",
      "74it [01:31,  1.00it/s]\u001b[A\n",
      "75it [01:33,  1.24s/it]\u001b[A\n",
      "76it [01:34,  1.05s/it]\u001b[A\n",
      "77it [01:36,  1.23s/it]\u001b[A\n",
      "78it [01:36,  1.04s/it]\u001b[A\n",
      "79it [01:38,  1.18s/it]\u001b[A\n",
      "80it [01:38,  1.02s/it]\u001b[A\n",
      "81it [01:40,  1.14s/it]\u001b[A\n",
      "82it [01:40,  1.03it/s]\u001b[A\n",
      "83it [01:42,  1.22s/it]\u001b[A\n",
      "84it [01:43,  1.03s/it]\u001b[A\n",
      "85it [01:44,  1.15s/it]\u001b[A\n",
      "86it [01:45,  1.05it/s]\u001b[A\n",
      "87it [01:46,  1.07s/it]\u001b[A\n",
      "88it [01:46,  1.09it/s]\u001b[A\n",
      "89it [01:48,  1.06s/it]\u001b[A\n",
      "90it [01:48,  1.08it/s]\u001b[A\n",
      "91it [01:50,  1.21s/it]\u001b[A\n",
      "92it [01:51,  1.01s/it]\u001b[A\n",
      "93it [01:52,  1.13s/it]\u001b[A\n",
      "94it [01:53,  1.04it/s]\u001b[A\n",
      "95it [01:54,  1.06s/it]\u001b[A\n",
      "96it [01:55,  1.10it/s]\u001b[A\n",
      "97it [01:56,  1.10s/it]\u001b[A\n",
      "98it [01:57,  1.05it/s]\u001b[A\n",
      "99it [01:59,  1.24s/it]\u001b[A\n",
      "100it [01:59,  1.04s/it]\u001b[A\n",
      "101it [02:01,  1.22s/it]\u001b[A\n",
      "102it [02:02,  1.02s/it]\u001b[A\n",
      "103it [02:03,  1.12s/it]\u001b[A\n",
      "104it [02:03,  1.05it/s]\u001b[A\n",
      "105it [02:05,  1.09s/it]\u001b[A\n",
      "106it [02:05,  1.06it/s]\u001b[A\n",
      "107it [02:07,  1.20s/it]\u001b[A\n",
      "108it [02:08,  1.02s/it]\u001b[A\n",
      "109it [02:09,  1.15s/it]\u001b[A\n",
      "110it [02:10,  1.03it/s]\u001b[A\n",
      "111it [02:11,  1.05s/it]\u001b[A\n",
      "112it [02:12,  1.11it/s]\u001b[A\n",
      "113it [02:13,  1.08s/it]\u001b[A\n",
      "114it [02:14,  1.07it/s]\u001b[A\n",
      "115it [02:16,  1.23s/it]\u001b[A\n",
      "116it [02:16,  1.04s/it]\u001b[A\n",
      "117it [02:18,  1.16s/it]\u001b[A\n",
      "118it [02:18,  1.02it/s]\u001b[A\n",
      "119it [02:20,  1.11s/it]\u001b[A\n",
      "120it [02:20,  1.05it/s]\u001b[A\n",
      "121it [02:22,  1.09s/it]\u001b[A\n",
      "122it [02:22,  1.06it/s]\u001b[A\n",
      "123it [02:24,  1.26s/it]\u001b[A\n",
      "124it [02:25,  1.03s/it]\u001b[A\n",
      "125it [02:26,  1.18s/it]\u001b[A\n",
      "126it [02:27,  1.00it/s]\u001b[A\n",
      "127it [02:28,  1.10s/it]\u001b[A\n",
      "128it [02:29,  1.04it/s]\u001b[A\n",
      "129it [02:30,  1.11s/it]\u001b[A\n",
      "130it [02:31,  1.02it/s]\u001b[A\n",
      "131it [02:33,  1.22s/it]\u001b[A\n",
      "132it [02:33,  1.03s/it]\u001b[A\n",
      "133it [02:35,  1.17s/it]\u001b[A\n",
      "134it [02:35,  1.01it/s]\u001b[A\n",
      "135it [02:37,  1.10s/it]\u001b[A\n",
      "136it [02:37,  1.06it/s]\u001b[A\n",
      "137it [02:39,  1.08s/it]\u001b[A\n",
      "138it [02:39,  1.06it/s]\u001b[A\n",
      "139it [02:41,  1.17s/it]\u001b[A\n",
      "140it [02:42,  1.00it/s]\u001b[A\n",
      "141it [02:43,  1.15s/it]\u001b[A\n",
      "142it [02:44,  1.02it/s]\u001b[A\n",
      "143it [02:45,  1.20s/it]\u001b[A\n",
      "144it [02:46,  1.00s/it]\u001b[A\n",
      "145it [02:47,  1.14s/it]\u001b[A\n",
      "146it [02:48,  1.02it/s]\u001b[A\n",
      "147it [02:50,  1.22s/it]\u001b[A\n",
      "148it [02:50,  1.03s/it]\u001b[A\n",
      "149it [02:52,  1.16s/it]\u001b[A\n",
      "150it [02:53,  1.02it/s]\u001b[A\n",
      "151it [02:54,  1.12s/it]\u001b[A\n",
      "152it [02:54,  1.05it/s]\u001b[A\n",
      "153it [02:56,  1.01s/it]\u001b[A\n",
      "154it [02:56,  1.19it/s]\u001b[A\n",
      "155it [02:57,  1.08it/s]\u001b[A\n",
      "156it [02:58,  1.31it/s]\u001b[A\n",
      "157it [02:58,  1.31it/s]\u001b[A\n",
      "158it [02:59,  1.54it/s]\u001b[A\n",
      "159it [02:59,  1.51it/s]\u001b[A\n",
      "160it [03:00,  1.72it/s]\u001b[A\n",
      "161it [03:01,  1.62it/s]\u001b[A\n",
      "162it [03:01,  1.82it/s]\u001b[A\n",
      "163it [03:02,  1.69it/s]\u001b[A\n",
      "164it [03:02,  1.88it/s]\u001b[A\n",
      "165it [03:03,  1.70it/s]\u001b[A\n",
      "166it [03:03,  1.90it/s]\u001b[A\n",
      "167it [03:04,  1.87it/s]\u001b[A/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DataParallel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MaxPool2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Identity. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Reshape. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ConvTranspose2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1, iter0, loss: 5.698641300201416\n",
      "Finish epoch 1, time elapsed 15.795316457748413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:08,  8.20s/it]\u001b[A\n",
      "2it [00:13,  7.39s/it]\u001b[A\n",
      "3it [00:15,  5.63s/it]\u001b[A\n",
      "4it [00:15,  4.10s/it]\u001b[A\n",
      "5it [00:16,  3.23s/it]\u001b[A\n",
      "6it [00:17,  2.41s/it]\u001b[A\n",
      "7it [00:18,  2.09s/it]\u001b[A\n",
      "8it [00:19,  1.64s/it]\u001b[A\n",
      "9it [00:21,  1.63s/it]\u001b[A\n",
      "10it [00:21,  1.35s/it]\u001b[A\n",
      "11it [00:23,  1.43s/it]\u001b[A\n",
      "12it [00:23,  1.18s/it]\u001b[A\n",
      "13it [00:25,  1.25s/it]\u001b[A\n",
      "14it [00:25,  1.05s/it]\u001b[A\n",
      "15it [00:27,  1.19s/it]\u001b[A\n",
      "16it [00:28,  1.04s/it]\u001b[A\n",
      "17it [00:29,  1.18s/it]\u001b[A\n",
      "18it [00:30,  1.03s/it]\u001b[A\n",
      "19it [00:31,  1.18s/it]\u001b[A\n",
      "20it [00:32,  1.00s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "def checkM(prev, q=False):\n",
    "    out = {}\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "                if obj.is_cuda and not q:\n",
    "                    name = str(obj.size())\n",
    "                    if name in out:\n",
    "                        out[name] += 1\n",
    "                    else:\n",
    "                        out[name] = 1\n",
    "                    \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    for key in out:\n",
    "        if key not in prev:\n",
    "            print(\"new: \" + key + \" : \" + str(out[key]))\n",
    "        elif prev[key] != out[key]:\n",
    "            #print(\"diff (new - old): \" + key + \" : \" + str(out[key] - prev[key]))\n",
    "            print(\"diff (new - old): \" + key + \" : \" + str(out[key])+ \" - \" +str(prev[key]))\n",
    "            \n",
    "    for key in prev:\n",
    "        if key not in out:\n",
    "            print(\"dropped: \" + key + \" : \" + str(prev[key]))\n",
    "    return out\n",
    "\n",
    "def memDiff(prev, out):\n",
    "    for key in out:\n",
    "        if key not in prev:\n",
    "            print(\"new: \" + key + \" : \" + str(out[key]))\n",
    "        elif prev[key] != out[key]:\n",
    "            print(\"diff (new - old): \" + key + \" : \" + str(out[key] - prev[key]))\n",
    "            \n",
    "    for key in prev:\n",
    "        if key not in out:\n",
    "            print(\"dropped: \" + key + \" : \" + str(prev[key]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 3\n",
    "    train_dataset = CityScapesDataset(csv_file='train.csv')\n",
    "    val_dataset = CityScapesDataset(csv_file='val.csv')\n",
    "    test_dataset = CityScapesDataset(csv_file='test.csv')\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=8,\n",
    "                          shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=8,\n",
    "                          shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=4,\n",
    "                          shuffle=True)\n",
    "    \n",
    "    \n",
    "    epochs     = 100\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # Fix magic number\n",
    "    model = getTransferModel(34)\n",
    "    \n",
    "    \n",
    "    epochs     = 100\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    train(model, criterion, epochs, train_loader, val_loader, test_loader, use_gpu, \"Transfer\")\n",
    "    \n",
    "    \n",
    "    model.load_state_dict(torch.load('./save_param'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "newfile = 'newfile999.pkl'\n",
    "a = 5\n",
    "b =99\n",
    "c = 100\n",
    "with open(newfile, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([a,b,c], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "with open(newfile, 'rb') as f:\n",
    "    [aac, bbc, ccc] = pickle.load(f)\n",
    "print(aac)\n",
    "print(bbc)\n",
    "print(ccc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
